# Repository Specification â€” AWS Transcription Workflow

**Tech Stack:** Terraform + JavaScript (Node.js)
**Goal:** Automate video upload â†’ splitting â†’ transcription â†’ subtitle storage (English & Spanish)

---

## ðŸ§© 1. Overview

This repository provisions and orchestrates an automated workflow on AWS that:

1. Uploads a video file to an S3 bucket.
2. Detects new uploads via an S3 event trigger.
3. Checks the file size.

   * If **â‰¤100 MB**, process directly.
   * If **>100 MB**, split it into smaller parts (â‰¤100 MB each).
4. Sends each resulting file (or segment) to **Amazon Transcribe** to generate subtitles in **English** and **Spanish**.
5. Stores the resulting `.srt` or `.vtt` subtitle files back into S3.

---

## ðŸ—ï¸ 2. Repository Structure

```
aws-transcribe-pipeline/
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ s3.tf
â”‚   â”œâ”€â”€ lambda.tf
â”‚   â”œâ”€â”€ iam.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ provider.tf
â”‚
â”œâ”€â”€ functions/
â”‚   â”œâ”€â”€ onUploadHandler.js
â”‚   â”œâ”€â”€ splitVideo.js
â”‚   â”œâ”€â”€ startTranscribe.js
â”‚   â”œâ”€â”€ monitorTranscribe.js
â”‚   â””â”€â”€ storeSubtitles.js
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ ffmpegLayer/
â”‚   â””â”€â”€ build.sh
â”‚
â”œâ”€â”€ package.json
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

---

## â˜ï¸ 3. AWS Resources (Terraform)

### 3.1 S3 Buckets

* **`video-uploads`**
  Receives the raw uploaded videos (from users or frontend).

  * Event: triggers Lambda `onUploadHandler` on new `.mp4` file.
  * Bucket policy grants `s3:GetObject`, `s3:PutObject` to Lambda roles.

* **`video-subtitles`**
  Stores generated subtitle files (`.vtt` or `.srt`) in structured folders:

  ```
  /{original_filename}/english.vtt
  /{original_filename}/spanish.vtt
  ```

---

### 3.2 Lambda Functions (Node.js)

| Function              | Trigger                                      | Description                                                                                                  |
| --------------------- | -------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **onUploadHandler**   | S3 (ObjectCreated)                           | Receives event from upload bucket. Checks file size and orchestrates logic.                                  |
| **splitVideo**        | Invoked by `onUploadHandler`                 | Uses `ffmpeg` to split videos >100 MB into multiple parts. Each part uploaded back to S3 (temporary folder). |
| **startTranscribe**   | Invoked by `onUploadHandler` or `splitVideo` | Starts Transcribe jobs for English and Spanish.                                                              |
| **monitorTranscribe** | CloudWatch Event or Step Function task       | Monitors job completion and triggers `storeSubtitles`.                                                       |
| **storeSubtitles**    | Invoked after transcription completes        | Downloads subtitles, stores them in `video-subtitles` bucket.                                                |

---

### 3.3 IAM Roles & Policies

Each Lambda function gets an execution role with:

* `AmazonS3FullAccess` (scoped to relevant buckets)
* `AmazonTranscribeFullAccess`
* `AWSLambdaBasicExecutionRole`
* Optionally: `CloudWatchLogsFullAccess`

---

### 3.4 AWS Transcribe Jobs

Created via AWS SDK (`@aws-sdk/client-transcribe`).
Each job includes:

```js
{
  TranscriptionJobName: `job-${fileId}-en`,
  LanguageCode: "en-US",
  Media: { MediaFileUri: s3FileUri },
  OutputBucketName: "video-subtitles",
  Subtitles: { Formats: ["vtt"], OutputStartIndex: 1 }
}
```

A second job with `LanguageCode: "es-ES"` is started after the English job, or in parallel.

---

### 3.5 Step Function (optional but recommended)

Instead of chaining Lambdas manually, define a **State Machine** that manages the workflow:

```
S3 Upload Event
    â†“
Check file size
    â†“
[ <100MB ] â”€â”€â”€â–º Start Transcribe (en + es)
[ >100MB ] â”€â”€â”€â–º Split Video â†’ Start Transcribe (per chunk)
    â†“
Wait for all jobs to complete
    â†“
Merge subtitles if split
    â†“
Store in S3
```

Terraform module can define this using `aws_sfn_state_machine`.

---

## ðŸ§  4. Function Details

### 4.1 `onUploadHandler.js`

**Responsibilities:**

* Receive S3 event
* Check file metadata and size
* Branch:

  * If â‰¤100 MB â†’ call `startTranscribe` directly
  * If >100 MB â†’ call `splitVideo` and process each chunk

**Example:**

```js
import { S3Client, HeadObjectCommand } from "@aws-sdk/client-s3";
import { startTranscribe } from "./startTranscribe.js";
import { splitVideo } from "./splitVideo.js";

export const handler = async (event) => {
  const s3 = new S3Client();
  const { bucket, key } = extractS3Info(event);

  const head = await s3.send(new HeadObjectCommand({ Bucket: bucket, Key: key }));
  const fileSizeMB = head.ContentLength / (1024 * 1024);

  if (fileSizeMB > 100) {
    await splitVideo(bucket, key);
  } else {
    await startTranscribe(bucket, key);
  }
};
```

---

### 4.2 `splitVideo.js`

**Responsibilities:**

* Download file (stream)
* Use `ffmpeg` (from Lambda Layer or Fargate container) to split by size or duration
* Upload each chunk to a `/chunks/` prefix in S3

**Example Split Command:**

```bash
ffmpeg -i input.mp4 -f segment -segment_time 300 -reset_timestamps 1 chunk_%03d.mp4
```

Then upload each chunk and call `startTranscribe` for each.

---

### 4.3 `startTranscribe.js`

**Responsibilities:**

* Submit transcription job(s) to Amazon Transcribe
* One job for English, one for Spanish
* Save job metadata to DynamoDB or pass to Step Function

---

### 4.4 `monitorTranscribe.js`

**Responsibilities:**

* Poll job status
* On completion, trigger `storeSubtitles`

---

### 4.5 `storeSubtitles.js`

**Responsibilities:**

* Read output subtitles from S3 Transcribe output bucket
* Rename/move to `video-subtitles/{file}/`
* Optionally merge multiple chunks into one `.vtt` file

---

## ðŸ§± 5. Terraform Modules Summary

| File           | Purpose                                                       |
| -------------- | ------------------------------------------------------------- |
| `provider.tf`  | AWS provider and region setup                                 |
| `variables.tf` | Bucket names, prefixes, Lambda configs                        |
| `s3.tf`        | Create input/output buckets and configure event notifications |
| `lambda.tf`    | Define Lambda functions, environment variables, and triggers  |
| `iam.tf`       | Create IAM roles and attach policies                          |
| `main.tf`      | Optional Step Function + outputs                              |
| `outputs.tf`   | Export ARNs, bucket names, etc.                               |

---

## ðŸ§ª 6. Local Development & Deployment

**Build & Deploy**

```bash
# Build Lambdas (zip)
npm run build

# Initialize Terraform
cd terraform
terraform init
terraform apply
```

**Environment Variables (Lambda)**

```
INPUT_BUCKET=video-uploads
OUTPUT_BUCKET=video-subtitles
TRANSCRIBE_ROLE_ARN=...
```

---

## ðŸ§© 7. Optional Enhancements

* âœ… **SNS notification** on transcript completion
* âœ… **DynamoDB table** for job tracking (status, timestamps, results)
* âœ… **Step Function visual workflow** for better observability
* âœ… **Add translation layer** if you want to automatically translate the English transcript into Spanish instead of dual transcription jobs
* âœ… **Integrate Amazon Translate + Subtitle merger** for multilingual subtitle generation

---

## ðŸš€ 8. Deployment Outcome

After deployment:

* Uploading a video to `video-uploads` bucket automatically:

  1. Triggers the workflow
  2. Splits large files
  3. Starts Transcribe jobs in English and Spanish
  4. Stores final subtitles in `video-subtitles` bucket
* No manual intervention needed.
